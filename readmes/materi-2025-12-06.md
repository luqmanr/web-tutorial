# Web Scraping dengan Python

## 1. Pengantar Web Scraping

### Apa itu Web Scraping?
Web scraping adalah proses mengekstraksi data dari situs web. Data ini biasanya data HTML yang tidak terstruktur, yang kemudian diubah menjadi data terstruktur yang dapat disimpan dan dianalisis dalam database, spreadsheet, atau format lainnya.

### Mengapa Web Scraping?
- Pengumpulan data untuk penelitian dan analisis
- Perbandingan harga
- Pembuatan prospek (Lead generation)
- Pemantauan aktivitas pesaing

### Apakah Web Scraping Legal dan Etis?
- Selalu periksa file `robots.txt` situs web (misalnya, `https://example.com/robots.txt`) untuk memahami kebijakan scraping mereka.
- Hormati `Syarat dan Ketentuan Layanan`.
- Hindari membebani server dengan terlalu banyak permintaan (gunakan penundaan).
- Bersikap transparan tentang aktivitas scraping Anda jika memungkinkan.

---

## 2. Bagaimana Browser Merender Halaman?

### Pipeline Rendering Browser
1.  **HTML** ‚Üí Pohon DOM
2.  **CSS** ‚Üí Pohon Render (gaya)
3.  **JavaScript** ‚Üí Manipulasi DOM (konten dinamis)
4.  **Permintaan Jaringan** ‚Üí AJAX / fetch ‚Üí HTML/JSON tambahan

*Halaman Statis*: data sudah ada di HTML ‚Üí mudah diurai.
*Halaman Dinamis*: data dimuat oleh JS ‚Üí memerlukan browser sungguhan atau panggilan API.

---

## 3. Ikhtisar Alat

| Alat           | Kekuatan                         | Penggunaan Umum                 |
| -------------- | -------------------------------- | ------------------------------- |
| **Requests**   | Klien HTTP cepat, ringan         | Halaman statis sederhana        |
| **BeautifulSoup4** | Mengurai HTML/XML, navigasi mudah  | Mengekstrak data dari DOM       |
| **Selenium**   | Otomatisasi browser sungguhan (Chrome/Firefox) | Berinteraksi dengan JS, klik, gulir |
| **lxml**       | Parsing cepat, dukungan XPath    | Saat kecepatan menjadi prioritas |

---

## 4. Konsep Dasar HTML untuk Scraping

### Struktur HTML
Halaman web dibangun dengan HTML (HyperText Markup Language), yang menggunakan struktur elemen (tag) seperti pohon.

```html
<!DOCTYPE html>
<html>
<head>
    <title>Halaman Saya</title>
</head>
<body>
    <h1 id="main-title" class="header">Selamat Datang</h1>
    <p class="content">Ini adalah sebuah paragraf.</p>
    <div class="container">
        <a href="https://example.com" class="link">Kunjungi Contoh</a>
    </div>
</body>
</html>
```

### Tag dan Elemen HTML
- `<h1>`, `<p>`, `<div>`, `<a>`, dll., adalah tag HTML.
- Sebuah elemen mencakup tag pembuka, konten, dan tag penutup.

### Atribut HTML: `id` dan `class`
- **`id`**: Pengenal unik untuk satu elemen HTML dalam sebuah halaman. Contoh: `<h1 id="main-title">`
- **`class`**: Pengenal non-unik yang dapat diterapkan ke beberapa elemen HTML. Digunakan untuk penataan gaya atau pengelompokan elemen. Contoh: `<p class="content">`, `<a class="link">`

---

## 5. Menginstal Paket

### Instalasi
```bash
# Paket inti
pip install selenium beautifulsoup4 requests lxml

# Driver Chrome (contoh)
# 1. Temukan versi browser Chrome Anda (chrome://version)
# 2. Unduh chromedriver yang sesuai dari https://chromedriver.chromium.org/
# 3. Letakkan di folder yang termasuk dalam $PATH atau tentukan jalur dalam kode
```

---

## 6. Mengurai HTML dengan BeautifulSoup4

BeautifulSoup4 adalah pustaka Python untuk mengurai dokumen HTML dan XML. Ini membuat pohon parse untuk halaman yang diurai yang dapat digunakan untuk mengekstrak data dari HTML.

### Penggunaan Dasar
```python
import requests
from bs4 import BeautifulSoup

url = "http://quotes.toscrape.com/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Temukan judul halaman
title = soup.find('h1').text
print(f"Judul Halaman: {title}")

# Temukan semua kutipan
quotes = soup.find_all('div', class_='quote')
for quote in quotes:
    text = quote.find('span', class_='text').text
    author = quote.find('small', class_='author').text
    print(f"\"{text}\" - {author}")
```

### Menjelajahi Pohon Parse
- `soup.tag`: Menemukan kemunculan pertama dari sebuah tag.
- `soup.find('tag', attributes)`: Menemukan tag yang cocok pertama dengan atribut opsional.
- `soup.find_all('tag', attributes)`: Menemukan semua tag yang cocok.
- `.text`: Mengekstrak konten teks dari sebuah elemen.
- `.get('attribute_name')`: Mengekstrak nilai dari sebuah atribut.

### Selektor gaya CSS (BeautifulSoup)
```python
# Semua elemen dengan kelas "product"
products = soup.select(".product")

# Selektor ID
main = soup.select_one("#main-content")

# Selektor atribut
imgs = soup.select("img[data-src]")
```

---

## 7. Apa itu XPath?

XPath (XML Path Language) adalah bahasa untuk memilih node dari dokumen XML (yang dapat diperlakukan sebagai HTML). Ini sangat kuat untuk menavigasi struktur HTML yang kompleks.

### Mengapa menggunakan XPath dibandingkan Selektor ID/Class/CSS?
Meskipun ID dan kelas sangat baik untuk pemilihan elemen secara langsung dan unik, XPath menawarkan fleksibilitas dan kekuatan yang lebih besar dalam skenario tertentu:

1.  **Menjelajahi Struktur Kompleks atau Dinamis**: XPath dapat melintasi DOM ke segala arah (induk, anak, saudara kandung, leluhur, keturunan), yang sangat penting ketika elemen tidak memiliki pengenal unik atau posisinya relatif terhadap elemen lain.
2.  **Memilih berdasarkan Konten Teks**: XPath memungkinkan Anda untuk memilih elemen berdasarkan konten teks yang terlihat (misalnya, `//button[text()='Submit']`), yang tidak mungkin dilakukan secara langsung dengan selektor CSS.
3.  **Tidak Adanya Atribut Unik**: Ketika elemen tidak memiliki atribut `id` atau `class` yang stabil (misalnya, ID yang dihasilkan secara dinamis), XPath dapat menemukannya menggunakan posisinya, atribut lain, atau hubungannya dengan elemen stabil.
4.  **Rantai dan Operasi Logika**: XPath mendukung kueri yang lebih kompleks dengan operator logika (`and`, `or`) dan fungsi (`contains()`, `starts-with()`, `ends-with()`) untuk menentukan elemen dengan tepat.
5.  **Memilih Elemen Saudara Kandung dan Induk**: XPath dapat dengan mudah memilih elemen induk atau saudara kandung berdasarkan elemen anak yang diketahui, yang sulit atau tidak mungkin dilakukan hanya dengan selektor CSS.

Pertimbangkan untuk menggunakan XPath ketika selektor CSS atau pemilihan berbasis ID/kelas menjadi terlalu kaku atau tidak dapat diandalkan karena sifat dinamis halaman web.

### Contoh Dasar XPath
- `//tagname`: Memilih semua elemen dengan nama tag yang ditentukan di mana saja dalam dokumen.
  - `//h1`: Memilih semua elemen `<h1>`.
- `//tagname[@attribute='value']`: Memilih elemen dengan atribut dan nilai tertentu.
  - `//p[@class='content']`: Memilih semua elemen `<p>` dengan `class="content"`
  - `//*[@id='main-title']`: Memilih elemen apa pun dengan `id="main-title"`
- `/html/body/div/a`: Jalur absolut dari root.
- `//div/a`: Memilih semua elemen `<a>` yang merupakan anak langsung dari `<div>`.
- `//div//a`: Memilih semua elemen `<a>` yang merupakan keturunan dari `<div>` (lebih dalam).

### Menggunakan XPath dengan lxml
```python
from lxml import html
import requests

page = requests.get("https://example.com").content
tree = html.fromstring(page)

# Mengekstrak semua judul produk
titles = tree.xpath("//h2[@class='product-title']/text()")
print(titles)
```

---

## 8. Berinteraksi dengan Konten yang Dirender JavaScript menggunakan Selenium

Scraper tradisional (seperti `requests` dan `BeautifulSoup4`) hanya mengambil HTML awal. Jika situs web menggunakan JavaScript untuk memuat konten secara dinamis, Anda memerlukan alat otomatisasi browser seperti Selenium.

### Apa itu Selenium? Mengapa & Kapan?
Selenium mengotomatiskan browser web. Ini memungkinkan Anda untuk mensimulasikan interaksi pengguna seperti mengklik tombol, mengisi formulir, dan menggulir, yang memicu JavaScript untuk merender konten.

- Konten halaman yang dihasilkan **setelah pemuatan halaman** (AJAX, gulir tak terbatas).
- Perlu **mengklik**, **mengisi formulir**, **menavigasi** melalui alur multi-langkah.
- Ingin **mengambil tangkapan layar** atau **mengunduh file** yang hanya ditampilkan setelah interaksi.

### Pengaturan Selenium (Chrome)
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time

chrome_options = Options()
# chrome_options.add_argument("--headless")          # jalankan tanpa UI untuk kinerja yang lebih baik
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")

# Pengaturan Chrome WebDriver
# Anda mungkin perlu menginstal webdriver_manager: pip install webdriver_manager
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=chrome_options)

url = "https://quotes.toscrape.com/js/" # Halaman yang dirender JavaScript
driver.get(url)
time.sleep(3) # Beri waktu halaman untuk memuat konten JavaScript

# Dapatkan sumber halaman setelah eksekusi JavaScript
html_content = driver.page_source
driver.quit()

from bs4 import BeautifulSoup
soup = BeautifulSoup(html_content, 'html.parser')

# Sekarang Anda dapat mengurai konten yang dimuat secara dinamis dengan BeautifulSoup
quotes = soup.find_all('div', class_='quote')
for quote in quotes:
    text = quote.find('span', class_='text').text
    author = quote.find('small', class_='author').text
    print(f"\"{text}\" - {author}")
```

### Locator Selenium

| Strategi        | Contoh                                                  |
| --------------- | -------------------------------------------------------- |
| **By.ID**       | `driver.find_element(By.ID, "search-box")`               |
| **By.NAME**     | `driver.find_element(By.NAME, "q")`                      |
| **By.CLASS_NAME** | `driver.find_element(By.CLASS_NAME, "nav-item")`         |
| **By.CSS_SELECTOR** | `driver.find_element(By.CSS_SELECTOR, "div.content > p")` |
| **By.XPATH**    | `driver.find_element(By.XPATH, "//button[text()='Submit']")` |

### Berinteraksi dengan JavaScript

```python
# Klik sebuah tombol
button = driver.find_element(By.XPATH, "//button[@id='load-more']")
button.click()

# Gulir ke bawah (memicu gulir tak terbatas)
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
```

- **`execute_script`** memungkinkan Anda menjalankan JS arbitrer dalam konteks halaman.
- Berguna untuk:
  - Memicu gambar yang dimuat secara malas (lazy-loaded).
  - Mengakses variabel yang disimpan di konsol browser.

### Mengekstrak Data setelah Eksekusi JS

```python
# Tunggu elemen muncul (explicit wait)
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

wait = WebDriverWait(driver, 10)
price_el = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".price")))

price = price_el.text
print("Harga:", price)

# Atau cukup baca HTML terakhir dan berikan ke BeautifulSoup
html = driver.page_source
soup = BeautifulSoup(html, "html.parser")
```

### Menangani Paginasi / Gulir Tak Terbatas

1.  **Paginasi statis** ‚Äì ikuti tautan ‚ÄúBerikutnya‚Äù.
    ```python
    while True:
        soup = BeautifulSoup(driver.page_source, "html.parser")
        # ‚Ä¶ ekstrak data ‚Ä¶
        try:
            next_btn = driver.find_element(By.LINK_TEXT, "Next")
            next_btn.click()
            wait.until(EC.staleness_of(next_btn))
        except Exception:
            break   # tidak ada halaman lagi
    ```

2.  **Gulir tak terbatas** ‚Äì terus gulir hingga tinggi tidak bertambah.

    ```python
    import time

    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
    ```

---

## 9. Menyimpan Data yang Di-scrape

### Mengekspor Hasil

| Format   | Cuplikan Python                                      |
| -------- | --------------------------------------------------- |
| **CSV**  | `import csv; csv.writer(open('out.csv','w')).writerows(data)` |
| **JSON** | `import json; json.dump(data, open('out.json','w'), indent=2)` |
| **SQLite** | `import sqlite3; conn = sqlite3.connect('data.db'); conn.executemany(...)` |

---

## 10. Praktik Terbaik untuk Web Scraping

### Pemecahan Masalah
- **Data hilang** ‚Üí halaman masih memuat ‚Üí tambahkan explicit waits.
- **Anti-scraping** ‚Üí CAPTCHA, larangan IP ‚Üí hormati `robots.txt`, tambahkan penundaan acak, rotasi user-agents / proxy.
- **ID dinamis** ‚Üí gunakan atribut stabil (class, data-*, text) atau XPath relatif.
- **Pembatasan laju** ‚Üí `time.sleep(random.uniform(1,3))`.
- **Deteksi headless** ‚Üí atur `userAgent`, nonaktifkan `navigator.webdriver`.

### Etika & Legalitas
- **Selalu periksa** `https://example.com/robots.txt`.
- **Baca Syarat & Ketentuan Layanan** situs ‚Äì beberapa secara eksplisit melarang scraping.
- **Jangan membebani** server; jaga permintaan tetap wajar.
- **Pertimbangkan API** ‚Äì banyak situs menyediakan titik akhir data resmi.

---

## 11. Demo Singkat (Opsional)

## Demo Langsung (5 menit)

Demo ini akan memanfaatkan `selenium/standalone-chrome` yang berjalan di Docker, memungkinkan kita untuk mengamati tindakan Selenium melalui antarmuka web noVNC.

1.  **Mulai Selenium Standalone Chrome dengan Docker (di terminal):**
    ```bash
    docker run -d -p 4444:4444 -p 7900:7900 --shm-size="2g" -e VNC_NO_PASSWORD=1 selenium/standalone-chrome:latest
    ```
    - `-d`: Jalankan dalam mode detached.
    - `-p 4444:4444`: Petakan port hub Selenium Grid.
    - `-p 7900:7900`: Petakan port noVNC untuk observasi browser.
    - `--shm-size="2g"`: Alokasikan memori bersama, seringkali diperlukan untuk Chrome di Docker.

2.  **Akses Antarmuka noVNC:**
    - Buka browser Anda dan kunjungi `http://localhost:7900/vnc.html`. Anda akan melihat browser Chrome yang dikendalikan oleh Selenium.

3.  **Jalankan skrip scraping Python Anda** (contoh di bawah, pastikan terhubung ke `http://localhost:4444/wd/hub`):
    ```python
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    import time

    # Konfigurasi opsi Chrome
    chrome_options = Options()
    # Jika berjalan di Docker, Anda mungkin tidak memerlukan --headless untuk observasi melalui noVNC
    # chrome_options.add_argument("--headless") # Komentar ini untuk demo visual

    # Terhubung ke Selenium Grid Hub yang berjalan di Docker
    driver = webdriver.Remote(
        command_executor='http://localhost:4444/wd/hub',
        options=chrome_options
    )

    try:
        url = "http://quotes.toscrape.com/scroll" # Contoh halaman dengan konten dinamis
        driver.get(url)
        time.sleep(3) # Beri waktu halaman untuk memuat

        # Contoh: Gulir ke bawah untuk memuat lebih banyak konten
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # Ekstrak data menggunakan BeautifulSoup setelah konten dinamis dimuat
        html_content = driver.page_source
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')

        quotes = soup.find_all('div', class_='quote')
        for i, quote in enumerate(quotes):
            text = quote.find('span', class_='text').text
            author = quote.find('small', class_='author').text
            print(f"Kutipan {i+1}: \"{text}\" - {author}")

    finally:
        driver.quit() # Selalu keluar dari driver
    ```

4.  **Amati Tindakan:** Perhatikan browser di jendela noVNC saat skrip Anda berjalan, menunjukkan interaksi waktu nyata.
5.  **Hentikan Kontainer Docker (di terminal setelah demo):**
    ```bash
    docker stop $(docker ps -q --filter ancestor=selenium/standalone-chrome)
    docker rm $(docker ps -aq --filter ancestor=selenium/standalone-chrome)
    ```

*Pengaturan ini memungkinkan debugging visual dan demonstrasi kemampuan Selenium.*

---

## 12. Sumber Daya Lebih Lanjut

### Pelajari Lebih Lanjut
- **Dokumentasi Selenium** ‚Äì https://www.selenium.dev/documentation/
- **Dokumentasi BeautifulSoup** ‚Äì https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **Scrapy (kerangka kerja berfitur lengkap)** ‚Äì https://scrapy.org/
- **Web Scraping Cheat Sheet** ‚Äì https://github.com/m0neywatcher/web-scraping-cheatsheet
- **Python Requests** ‚Äì https://docs.python-requests.org/

*Selamat scraping! üéâ*